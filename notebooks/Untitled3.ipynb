{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f23244c-edf4-4585-b641-41bb1ca1bb87",
   "metadata": {},
   "source": [
    "# Country → Subbasins → Predictors: Data Prep for WASS2SHydroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81240aab-4057-4405-a9b8-126203e990db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using GitHub PAT from the git credential store.\n",
      "\n",
      "Skipping install of 'WASS2SHydroR' from a github remote, the SHA1 (daae2af1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devtools::install_github(\"kiema97/AGRHYMET-WASS2SHydroR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ea6e47-fc05-44d3-ba08-dfb14084d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PARAMETERS (participants only edit this block) ==========================\n",
    "COUNTRY_CODE <- \"BFA\" \n",
    "PREDICTOR_VARS <- c(\"PRCP\", \"SST\")  # choose among available folders under predictors/\n",
    "SELECTED_MODELS <- NULL  # e.g., c(\"CanCM3\",\"CCSM4\") or NULL to use all available\n",
    "SELECTED_MODELS <- c(\"CCSM4\",\"CanCM3\")\n",
    "# Where things live (relative to project root)\n",
    "PATH_COUNTRIES   <- \"static/was_contries.shp\"   # shapefile with GMI_CNTRY field\n",
    "PATH_SUBBASINS   <- \"static/subbassins.shp\"     # shapefile with HYBAS_ID field\n",
    "PATH_HISTORICAL  <- \"data/was_subbassins_data.csv\"  # columns: DATE, HYBAS_ID, Q, prcp, evap\n",
    "PATH_PREDICTORS  <- \"D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep\"             # expect subfolders: PRCP/, SST/\n",
    "setwd(\"D:/CCR_AOS/WASS2SHydroRTraining\")\n",
    "# Optional: performance/speed knobs\n",
    "N_CORES <- 4#max(1, parallel::detectCores() - 1)\n",
    "\n",
    "# ==== Libraries ==============================================================\n",
    "suppressPackageStartupMessages({\n",
    "  library(dplyr)\n",
    "  library(readr)\n",
    "  library(stringr)\n",
    "  library(purrr)\n",
    "  library(tidyr)\n",
    "  library(sf)\n",
    "  library(lubridate)\n",
    "})\n",
    "\n",
    "# WASS2SHydroR is expected to be installed; falls back gracefully if missing\n",
    "has_wass2s <- requireNamespace(\"WASS2SHydroR\", quietly = TRUE)\n",
    "if (!has_wass2s) {\n",
    "  message(\"NOTE: Package 'WASS2SHydroR' not found. You can still run most steps;\\n\",\n",
    "          \"the function 'wass2s_prepare_data()' will be called only if available.\")\n",
    "}\n",
    "\n",
    "# Helper: safe parallel plan (base R's parallel via mclapply on Unix; fall back on lapply on Windows)\n",
    ".parallel_map <- function(X, FUN, ...){\n",
    "  if (.Platform$OS.type == \"windows\") {\n",
    "    # Simple fallback for Windows notebooks to avoid cluster overhead for trainees\n",
    "    lapply(X, FUN, ...)\n",
    "  } else {\n",
    "    parallel::mclapply(X, FUN, mc.cores = N_CORES, ...)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef26f0-f0aa-4be7-9f6b-6f6569fd3862",
   "metadata": {},
   "source": [
    "## 1) Select the user's country and find covered subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0aca10b-29d3-400f-9bf8-10649db4f5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"attribute variables are assumed to be spatially constant throughout all geometries\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "10"
      ],
      "text/latex": [
       "10"
      ],
      "text/markdown": [
       "10"
      ],
      "text/plain": [
       "[1] 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopifnot(length(COUNTRY_CODE) == 1, nchar(COUNTRY_CODE) == 3)\n",
    "\n",
    "# Read shapefiles\n",
    "a_countries <- sf::st_read(PATH_COUNTRIES, quiet = TRUE) %>% \n",
    "  sf::st_make_valid()\n",
    "a_subs      <- sf::st_read(PATH_SUBBASINS, quiet = TRUE) %>% \n",
    "  sf::st_make_valid()\n",
    "\n",
    "# Ensure same CRS\n",
    "if (sf::st_crs(a_countries) != sf::st_crs(a_subs)) {\n",
    "  a_subs <- sf::st_transform(a_subs, sf::st_crs(a_countries))\n",
    "}\n",
    "\n",
    "# Filter country\n",
    "country <- a_countries %>% filter(.data$GMI_CNTRY == COUNTRY_CODE)\n",
    "if (nrow(country) == 0) stop(\"No country with GMI_CNTRY == \", COUNTRY_CODE)\n",
    "\n",
    "# Intersections: subbasins partially or fully covered by the country polygon\n",
    "inter_idx <- sf::st_intersects(a_subs, country, sparse = TRUE)\n",
    "sel <- lengths(inter_idx) > 0\n",
    "subs_sel <- a_subs[sel, ]\n",
    "\n",
    "# Classify as FULL vs PARTIAL coverage (by area ratio of intersection)\n",
    "inter_geom <- sf::st_intersection(sf::st_make_valid(subs_sel), sf::st_make_valid(country))\n",
    "area_sub   <- sf::st_area(subs_sel)\n",
    "area_int   <- sf::st_area(inter_geom)\n",
    "cover      <- as.numeric(area_int) / as.numeric(area_sub)\n",
    "\n",
    "subs_sel$coverage_class <- ifelse(cover >= 0.999, \"FULL\", \"PARTIAL\")\n",
    "subs_sel$coverage_ratio <- cover\n",
    "\n",
    "HYBAS_IDS <- subs_sel$HYBAS_ID\n",
    "length(HYBAS_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84e14ed-db9f-4f61-ace2-275b5a101088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 10 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>HYBAS_ID</th><th scope=col>coverage_class</th><th scope=col>coverage_ratio</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1040786860</td><td>FULL   </td><td>1.0000000</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1040709280</td><td>PARTIAL</td><td>0.9784857</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1040786690</td><td>PARTIAL</td><td>0.9448550</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1040873650</td><td>PARTIAL</td><td>0.5413947</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1040709030</td><td>PARTIAL</td><td>0.4691100</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1040915990</td><td>PARTIAL</td><td>0.2868057</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>1040023890</td><td>PARTIAL</td><td>0.2122455</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>1040722720</td><td>PARTIAL</td><td>0.1709675</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>1040873640</td><td>PARTIAL</td><td>0.1455303</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>1040641670</td><td>PARTIAL</td><td>0.0452270</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 10 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & HYBAS\\_ID & coverage\\_class & coverage\\_ratio\\\\\n",
       "  & <dbl> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1040786860 & FULL    & 1.0000000\\\\\n",
       "\t2 & 1040709280 & PARTIAL & 0.9784857\\\\\n",
       "\t3 & 1040786690 & PARTIAL & 0.9448550\\\\\n",
       "\t4 & 1040873650 & PARTIAL & 0.5413947\\\\\n",
       "\t5 & 1040709030 & PARTIAL & 0.4691100\\\\\n",
       "\t6 & 1040915990 & PARTIAL & 0.2868057\\\\\n",
       "\t7 & 1040023890 & PARTIAL & 0.2122455\\\\\n",
       "\t8 & 1040722720 & PARTIAL & 0.1709675\\\\\n",
       "\t9 & 1040873640 & PARTIAL & 0.1455303\\\\\n",
       "\t10 & 1040641670 & PARTIAL & 0.0452270\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 10 × 3\n",
       "\n",
       "| <!--/--> | HYBAS_ID &lt;dbl&gt; | coverage_class &lt;chr&gt; | coverage_ratio &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | 1040786860 | FULL    | 1.0000000 |\n",
       "| 2 | 1040709280 | PARTIAL | 0.9784857 |\n",
       "| 3 | 1040786690 | PARTIAL | 0.9448550 |\n",
       "| 4 | 1040873650 | PARTIAL | 0.5413947 |\n",
       "| 5 | 1040709030 | PARTIAL | 0.4691100 |\n",
       "| 6 | 1040915990 | PARTIAL | 0.2868057 |\n",
       "| 7 | 1040023890 | PARTIAL | 0.2122455 |\n",
       "| 8 | 1040722720 | PARTIAL | 0.1709675 |\n",
       "| 9 | 1040873640 | PARTIAL | 0.1455303 |\n",
       "| 10 | 1040641670 | PARTIAL | 0.0452270 |\n",
       "\n"
      ],
      "text/plain": [
       "   HYBAS_ID   coverage_class coverage_ratio\n",
       "1  1040786860 FULL           1.0000000     \n",
       "2  1040709280 PARTIAL        0.9784857     \n",
       "3  1040786690 PARTIAL        0.9448550     \n",
       "4  1040873650 PARTIAL        0.5413947     \n",
       "5  1040709030 PARTIAL        0.4691100     \n",
       "6  1040915990 PARTIAL        0.2868057     \n",
       "7  1040023890 PARTIAL        0.2122455     \n",
       "8  1040722720 PARTIAL        0.1709675     \n",
       "9  1040873640 PARTIAL        0.1455303     \n",
       "10 1040641670 PARTIAL        0.0452270     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick table for trainees\n",
    "subs_sel %>% \n",
    "  st_drop_geometry() %>% \n",
    "  arrange(desc(coverage_ratio)) %>% \n",
    "  select(HYBAS_ID, coverage_class, coverage_ratio) %>% \n",
    "  head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0beb20c-c852-4d12-a4ef-55faa0331e69",
   "metadata": {},
   "source": [
    "## 2) Load historical data (HYPE-simulated Q) for selected subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9705725-752b-4132-ae62-4e4f18d55a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 10 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>HYBAS_ID</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1040023890</td><td>16222</td></tr>\n",
       "\t<tr><td>1040641670</td><td>16222</td></tr>\n",
       "\t<tr><td>1040709030</td><td>16222</td></tr>\n",
       "\t<tr><td>1040709280</td><td>16222</td></tr>\n",
       "\t<tr><td>1040722720</td><td>16222</td></tr>\n",
       "\t<tr><td>1040786690</td><td>16222</td></tr>\n",
       "\t<tr><td>1040786860</td><td>16222</td></tr>\n",
       "\t<tr><td>1040873640</td><td>16222</td></tr>\n",
       "\t<tr><td>1040873650</td><td>16222</td></tr>\n",
       "\t<tr><td>1040915990</td><td>16222</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 10 × 2\n",
       "\\begin{tabular}{ll}\n",
       " HYBAS\\_ID & n\\\\\n",
       " <dbl> & <int>\\\\\n",
       "\\hline\n",
       "\t 1040023890 & 16222\\\\\n",
       "\t 1040641670 & 16222\\\\\n",
       "\t 1040709030 & 16222\\\\\n",
       "\t 1040709280 & 16222\\\\\n",
       "\t 1040722720 & 16222\\\\\n",
       "\t 1040786690 & 16222\\\\\n",
       "\t 1040786860 & 16222\\\\\n",
       "\t 1040873640 & 16222\\\\\n",
       "\t 1040873650 & 16222\\\\\n",
       "\t 1040915990 & 16222\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 10 × 2\n",
       "\n",
       "| HYBAS_ID &lt;dbl&gt; | n &lt;int&gt; |\n",
       "|---|---|\n",
       "| 1040023890 | 16222 |\n",
       "| 1040641670 | 16222 |\n",
       "| 1040709030 | 16222 |\n",
       "| 1040709280 | 16222 |\n",
       "| 1040722720 | 16222 |\n",
       "| 1040786690 | 16222 |\n",
       "| 1040786860 | 16222 |\n",
       "| 1040873640 | 16222 |\n",
       "| 1040873650 | 16222 |\n",
       "| 1040915990 | 16222 |\n",
       "\n"
      ],
      "text/plain": [
       "   HYBAS_ID   n    \n",
       "1  1040023890 16222\n",
       "2  1040641670 16222\n",
       "3  1040709030 16222\n",
       "4  1040709280 16222\n",
       "5  1040722720 16222\n",
       "6  1040786690 16222\n",
       "7  1040786860 16222\n",
       "8  1040873640 16222\n",
       "9  1040873650 16222\n",
       "10 1040915990 16222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_df <- readr::read_csv(PATH_HISTORICAL, show_col_types = FALSE) %>%\n",
    "  mutate(DATE = as.Date(.data$DATE)) %>%\n",
    "  filter(.data$HYBAS_ID %in% HYBAS_IDS) %>%\n",
    "  arrange(.data$HYBAS_ID, .data$DATE)\n",
    "\n",
    "# Sanity check\n",
    "hist_df %>% group_by(HYBAS_ID) %>% summarise(n = n(), .groups = \"drop\") %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cea082-9e92-4581-8edc-06a0336d3665",
   "metadata": {},
   "source": [
    "## 3) Catalog available predictor files (PRCP / SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7635a50c-9271-4a60-90a3-6d64260929f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 3 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>model</th><th scope=col>var</th><th scope=col>init_year</th><th scope=col>file</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>CCSM4 </td><td>PRCP</td><td>NA</td><td>D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc      </td></tr>\n",
       "\t<tr><td>CCSM4 </td><td>PRCP</td><td>NA</td><td>D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc</td></tr>\n",
       "\t<tr><td>CanCM3</td><td>PRCP</td><td>NA</td><td>D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 3 × 4\n",
       "\\begin{tabular}{llll}\n",
       " model & var & init\\_year & file\\\\\n",
       " <chr> & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t CCSM4  & PRCP & NA & D:/CCR\\_AOS/Wass2sHydro-Training\\_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc      \\\\\n",
       "\t CCSM4  & PRCP & NA & D:/CCR\\_AOS/Wass2sHydro-Training\\_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP\\_f2025.nc\\\\\n",
       "\t CanCM3 & PRCP & NA & D:/CCR\\_AOS/Wass2sHydro-Training\\_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 3 × 4\n",
       "\n",
       "| model &lt;chr&gt; | var &lt;chr&gt; | init_year &lt;int&gt; | file &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| CCSM4  | PRCP | NA | D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc       |\n",
       "| CCSM4  | PRCP | NA | D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc |\n",
       "| CanCM3 | PRCP | NA | D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc      |\n",
       "\n"
      ],
      "text/plain": [
       "  model  var  init_year\n",
       "1 CCSM4  PRCP NA       \n",
       "2 CCSM4  PRCP NA       \n",
       "3 CanCM3 PRCP NA       \n",
       "  file                                                                                 \n",
       "1 D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc      \n",
       "2 D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
       "3 D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Walk predictor folders and parse filenames like: CanCM3.PRCP.nc, CCSM4.PRCP_f2025.nc, CCSM4.SST.nc\n",
    "catalog_predictors <- function(base_dir = PATH_PREDICTORS){\n",
    "  dirs <- list.dirs(base_dir, full.names = TRUE, recursive = FALSE)\n",
    "  tibble(dir = dirs) %>%\n",
    "    mutate(var = basename(dir)) %>%\n",
    "    filter(var %in% c(\"PRCP\",\"SST\")) %>%\n",
    "    mutate(files = map(dir, ~list.files(.x, pattern = \"\\\\.nc$\", full.names = TRUE))) %>%\n",
    "    tidyr::unnest(files) %>%\n",
    "    mutate(\n",
    "      file = files,\n",
    "      fname = basename(file),\n",
    "      # Patterns: Model.VAR.nc OR Model.VAR_fYYYY.nc\n",
    "      model = str_replace(fname, \"^([^.]+)\\\\..*$\", \"\\\\1\"),\n",
    "      var_detect = toupper(str_replace(fname, \"^[^.]+\\\\.([A-Za-z]+).*$\", \"\\\\1\")),\n",
    "      init_year = ifelse(str_detect(fname, \"_f(\\\\\\\n",
    "\\\\d{4})\"), as.integer(str_replace(fname, \".*_f(\\\\\\\n",
    "\\\\d{4}).*\", \"\\\\1\")), NA_integer_),\n",
    "      var = ifelse(var_detect %in% c(\"PRCP\",\"SST\"), var_detect, var)\n",
    "    ) %>%\n",
    "    select(model, var, init_year, file)\n",
    "}\n",
    "\n",
    "pred_catalog <- catalog_predictors()\n",
    "\n",
    "# Filter by trainee choices\n",
    "pred_catalog <- pred_catalog %>% filter(var %in% PREDICTOR_VARS)\n",
    "if (!is.null(SELECTED_MODELS)) {\n",
    "  pred_catalog <- pred_catalog %>% filter(model %in% SELECTED_MODELS)\n",
    "}\n",
    "\n",
    "pred_catalog %>% arrange(var, model, init_year) %>% head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6642649-b9b4-490e-be41-d5b37cfb790b",
   "metadata": {},
   "source": [
    "## 4) Convert NetCDF → data frame **per model** and **per subbasin** using `wass2s_prepare_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f23d3df-4404-4ac8-a7d2-8b025de07013",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CanCM3.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP.nc\n",
      "\n",
      "[read] D:/CCR_AOS/Wass2sHydro-Training_base/predictors/PRCP/Jun-Sep/PRCP/CCSM4.PRCP_f2025.nc\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "10"
      ],
      "text/latex": [
       "10"
      ],
      "text/markdown": [
       "10"
      ],
      "text/plain": [
       "[1] 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (!has_wass2s) {\n",
    "  stop(\"'WASS2SHydroR' is required for NetCDF → data frame conversion. Please install it.\")\n",
    "}\n",
    "\n",
    "# ---- Integration notes -------------------------------------------------------\n",
    "# We build, for EACH HYBAS_ID, a **list of data.frames**, where each data.frame\n",
    "# corresponds to ONE climate model (e.g., CanCM3), and contains:\n",
    "#   HYBAS_ID, DATE, Q, and predictor columns renamed to the generic pattern\n",
    "#   expected by WASS2SHydroR (prcp_1, prcp_2, ..., sst_1, ...).\n",
    "#\n",
    "# 'wass2s_prepare_data()' is assumed to:\n",
    "#   - take a NetCDF file path and a polygon geometry (subbasin),\n",
    "#   - return a data.frame with at least 'DATE' and pixel/feature columns.\n",
    "\n",
    "# Keep only predictor columns (exclude DATE)\n",
    ".pick_predictor_cols <- function(df){\n",
    "  setdiff(names(df), c(\"DATE\", \"date\", \"Date\"))\n",
    "}\n",
    "\n",
    "# Aggregate pixel columns to a single series (mean by default) and name it\n",
    ".aggregate_pixels <- function(df, var_name){\n",
    "  px_cols <- .pick_predictor_cols(df)\n",
    "  if (length(px_cols) == 0) return(NULL)\n",
    "  out <- df %>% mutate(DATE = as.Date(.data$DATE)) %>%\n",
    "    mutate(!!var_name := rowMeans(across(all_of(px_cols)), na.rm = TRUE)) %>%\n",
    "    select(DATE, all_of(var_name))\n",
    "  out\n",
    "}\n",
    "\n",
    "# Extract one variable (PRCP or SST) for one (HYBAS_ID, nc_file)\n",
    "extract_var_for_subbasin <- function(hybas_id, nc_path, var_name){\n",
    "  geom <- subs_sel %>% filter(.data$HYBAS_ID == hybas_id) %>% sf::st_geometry()\n",
    "  if (length(geom) == 0) return(NULL)\n",
    "  bbox <- sf::st_bbox(geom)[c(4,1,2,3)]\n",
    "  df_raw <- WASS2SHydroR::wass2s_prepare_data(x = nc_path, bbox = bbox,cell_layout = \"wide\")\n",
    "  # If already feature-engineered (e.g., var_name or var_name_1 present), keep it\n",
    "  ready_cols <- names(df_raw)[names(df_raw) %in% c(var_name, paste0(var_name, \"_1\"))]\n",
    "  if (length(ready_cols) >= 1) {\n",
    "    sel <- c(\"DATE\", ready_cols[1])\n",
    "    out <- df_raw[, sel, drop = FALSE]\n",
    "    names(out)[2] <- var_name\n",
    "    return(out)\n",
    "  }\n",
    "  # Else aggregate pixel columns\n",
    "  .aggregate_pixels(df_raw, var_name)\n",
    "}\n",
    "\n",
    "# Build predictors (wide) for ONE model and ONE subbasin\n",
    "# files_tbl_one_model: rows for a single 'model', columns: model, var, init_year, file\n",
    "build_predictor_block_for_model <- function(hybas_id, files_tbl_one_model){\n",
    "  stopifnot(length(unique(files_tbl_one_model$model)) == 1)\n",
    "  pieces <- vector(\"list\", nrow(files_tbl_one_model))\n",
    "  for (i in seq_len(nrow(files_tbl_one_model))){\n",
    "    row <- files_tbl_one_model[i,]\n",
    "    var_name <- tolower(row$var)  # \"prcp\" or \"sst\"\n",
    "    tmp <- extract_var_for_subbasin(hybas_id, row$file, var_name)\n",
    "    if (!is.null(tmp)) pieces[[i]] <- tmp\n",
    "  }\n",
    "  pieces <- pieces[lengths(pieces) > 0]\n",
    "  if (length(pieces) == 0) return(NULL)\n",
    "  Reduce(function(x, y) dplyr::full_join(x, y, by = \"DATE\"), pieces) %>% arrange(DATE)\n",
    "}\n",
    "\n",
    "# Harmonize predictor names to prcp_1, prcp_2, ..., sst_1, ... (per MODEL)\n",
    "remap_predictor_names <- function(df){\n",
    "    return(df)\n",
    "  preds <- setdiff(names(df), c(\"HYBAS_ID\",\"DATE\",\"Q\"))\n",
    "  prcp_cols <- preds[str_detect(preds, \"(^|_)prcp(\\b|_)\")]\n",
    "  sst_cols  <- preds[str_detect(preds,  \"(^|_)sst(\\b|_)\")]\n",
    "  prcp_cols <- sort(prcp_cols)\n",
    "  sst_cols  <- sort(sst_cols)\n",
    "  ren <- c(setNames(paste0(\"prcp_\", seq_along(prcp_cols)), prcp_cols),\n",
    "           setNames(paste0(\"sst_\",  seq_along(sst_cols )),  sst_cols ))\n",
    "  dplyr::rename(df, !!!ren)\n",
    "}\n",
    "\n",
    "# Build the final NESTED TRAINING LIST expected by the new design:\n",
    "# training_nested[[HYBAS_ID]][[MODEL]] = data.frame(HYBAS_ID, DATE, Q, prcp_1..., sst_1...)\n",
    "make_training_nested_list <- function(hist, pred_tbl){\n",
    "  by_hybas <- split(hist, hist$HYBAS_ID)\n",
    "  models   <- sort(unique(pred_tbl$model))\n",
    "  res <- .parallel_map(names(by_hybas), function(hid){\n",
    "    hdf <- by_hybas[[hid]] %>% select(DATE, HYBAS_ID, Q)\n",
    "    # For each model, assemble its predictors then join with Q\n",
    "    per_model <- lapply(models, function(m){\n",
    "      files_m <- pred_tbl %>% filter(model == m)\n",
    "      pred_m  <- build_predictor_block_for_model(as.integer(hid), files_m)\n",
    "      if (is.null(pred_m)) return(NULL)\n",
    "      out <- dplyr::left_join(hdf, pred_m, by = \"DATE\")\n",
    "      # Reorder columns\n",
    "      pred_cols <- setdiff(names(out), c(\"HYBAS_ID\",\"DATE\",\"Q\"))\n",
    "      out <- out %>% select(HYBAS_ID, DATE, Q, all_of(pred_cols))\n",
    "      # Remap to prcp_1, sst_1, ... (model-specific)\n",
    "      out <- remap_predictor_names(out)\n",
    "      out\n",
    "    })\n",
    "    names(per_model) <- models\n",
    "    # Drop empty models for this HYBAS_ID\n",
    "    per_model[!vapply(per_model, is.null, logical(1))]\n",
    "  })\n",
    "  names(res) <- names(by_hybas)\n",
    "  # Drop HYBAS_ID with no models\n",
    "  res <- res[vapply(res, function(x) length(x) > 0, logical(1))]\n",
    "  res\n",
    "}\n",
    "\n",
    "training_nested <- make_training_nested_list(hist_df, pred_catalog)\n",
    "length(training_nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0852e2c-7ed9-4964-80da-03e911eb2069",
   "metadata": {},
   "source": [
    "## 5) Quick sanity checks on the nested output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72c4cca4-59d9-4174-b46c-11c4c61faac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$n_hybas\n",
       "[1] 10\n",
       "\n",
       "$summary_models_per_hybas\n",
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "      2       2       2       2       2       2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>HYBAS_ID</th><th scope=col>DATE</th><th scope=col>Q</th><th scope=col>prcp</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1040023890</td><td>1981-01-01</td><td>124.057</td><td>NA</td></tr>\n",
       "\t<tr><td>1040023890</td><td>1981-01-02</td><td>119.591</td><td>NA</td></tr>\n",
       "\t<tr><td>1040023890</td><td>1981-01-03</td><td>115.742</td><td>NA</td></tr>\n",
       "\t<tr><td>1040023890</td><td>1981-01-04</td><td>112.177</td><td>NA</td></tr>\n",
       "\t<tr><td>1040023890</td><td>1981-01-05</td><td>108.763</td><td>NA</td></tr>\n",
       "\t<tr><td>1040023890</td><td>1981-01-06</td><td>105.476</td><td>NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " HYBAS\\_ID & DATE & Q & prcp\\\\\n",
       " <dbl> & <date> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1040023890 & 1981-01-01 & 124.057 & NA\\\\\n",
       "\t 1040023890 & 1981-01-02 & 119.591 & NA\\\\\n",
       "\t 1040023890 & 1981-01-03 & 115.742 & NA\\\\\n",
       "\t 1040023890 & 1981-01-04 & 112.177 & NA\\\\\n",
       "\t 1040023890 & 1981-01-05 & 108.763 & NA\\\\\n",
       "\t 1040023890 & 1981-01-06 & 105.476 & NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 4\n",
       "\n",
       "| HYBAS_ID &lt;dbl&gt; | DATE &lt;date&gt; | Q &lt;dbl&gt; | prcp &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1040023890 | 1981-01-01 | 124.057 | NA |\n",
       "| 1040023890 | 1981-01-02 | 119.591 | NA |\n",
       "| 1040023890 | 1981-01-03 | 115.742 | NA |\n",
       "| 1040023890 | 1981-01-04 | 112.177 | NA |\n",
       "| 1040023890 | 1981-01-05 | 108.763 | NA |\n",
       "| 1040023890 | 1981-01-06 | 105.476 | NA |\n",
       "\n"
      ],
      "text/plain": [
       "  HYBAS_ID   DATE       Q       prcp\n",
       "1 1040023890 1981-01-01 124.057 NA  \n",
       "2 1040023890 1981-01-02 119.591 NA  \n",
       "3 1040023890 1981-01-03 115.742 NA  \n",
       "4 1040023890 1981-01-04 112.177 NA  \n",
       "5 1040023890 1981-01-05 108.763 NA  \n",
       "6 1040023890 1981-01-06 105.476 NA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print structure: how many HYBAS_ID and how many models per HYBAS_ID\n",
    "n_hybas <- length(training_nested)\n",
    "models_per <- vapply(training_nested, length, integer(1))\n",
    "list(n_hybas = n_hybas, summary_models_per_hybas = summary(models_per))\n",
    "\n",
    "# Show head of first HYBAS_ID / first MODEL\n",
    "first_h <- names(training_nested)[1]\n",
    "if (!is.null(first_h)) {\n",
    "  first_m <- names(training_nested[[first_h]])[1]\n",
    "  if (!is.null(first_m)) {\n",
    "    training_nested[[first_h]][[first_m]] %>% head()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35f891-1584-4703-9ea3-0a830e37e91b",
   "metadata": {},
   "source": [
    "## 6) (Optional) Filter variables or limit counts per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda14fc-e1ea-4780-8b51-54fe2e840da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{r country-subbasins, message=FALSE}\n",
    "\n",
    "```\n",
    "\n",
    "```{r view-subsel}\n",
    "\n",
    "```\n",
    "\n",
    "## 2) Load historical data (HYPE-simulated Q) for selected subbasins\n",
    "\n",
    "```{r historical}\n",
    "\n",
    "```\n",
    "\n",
    "## 3) Catalog available predictor files (PRCP / SST)\n",
    "\n",
    "```{r catalog}\n",
    "\n",
    "```\n",
    "\n",
    "## 4) Convert NetCDF → data frame **per model** and **per subbasin** using `wass2s_prepare_data()`\n",
    "\n",
    "```{r nc-to-df, message=FALSE}\n",
    "\n",
    "```\n",
    "\n",
    "## 5) Quick sanity checks on the nested output\n",
    "\n",
    "```{r checks}\n",
    "# Print structure: how many HYBAS_ID and how many models per HYBAS_ID\n",
    "n_hybas <- length(training_nested)\n",
    "models_per <- vapply(training_nested, length, integer(1))\n",
    "list(n_hybas = n_hybas, summary_models_per_hybas = summary(models_per))\n",
    "\n",
    "# Show head of first HYBAS_ID / first MODEL\n",
    "first_h <- names(training_nested)[1]\n",
    "if (!is.null(first_h)) {\n",
    "  first_m <- names(training_nested[[first_h]])[1]\n",
    "  if (!is.null(first_m)) {\n",
    "    training_nested[[first_h]][[first_m]] %>% head()\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6) (Optional) Filter variables or limit counts per model\n",
    "\n",
    "```{r optional-limit}\n",
    "# Example: keep only first 3 PRCP and first 2 SST columns per model\n",
    "limit_predictors <- function(df, k_prcp = 3, l_sst = 2){\n",
    "  preds <- setdiff(names(df), c(\"HYBAS_ID\",\"DATE\",\"Q\"))\n",
    "  keep <- c(\n",
    "    preds[str_detect(preds, \"^prcp_\\\")][seq_len(min(sum(str_detect(preds, \"^prcp_\\\")), k_prcp))],\n",
    "    preds[str_detect(preds,  \"^sst_\\\")][seq_len(min(sum(str_detect(preds,  \"^sst_\\\")), l_sst ))]\n",
    "  )\n",
    "  base <- c(\"HYBAS_ID\",\"DATE\",\"Q\")\n",
    "  intersect(c(base, keep), names(df)) %>% dplyr::select(df, all_of(.))\n",
    "}\n",
    "\n",
    "# Apply if desired (disabled by default)\n",
    "# training_nested <- lapply(training_nested, function(per_model){\n",
    "#   lapply(per_model, limit_predictors, k_prcp = 3, l_sst = 2)\n",
    "# })\n",
    "```\n",
    "\n",
    "## 7) Save the prepared nested list for modeling\n",
    "\n",
    "```{r save}\n",
    "dir.create(\"outputs\", showWarnings = FALSE)\n",
    "saveRDS(training_nested, file = file.path(\"outputs\", paste0(\"training_nested_\", COUNTRY_CODE, \".rds\")))\n",
    "message(\"Saved: \", file.path(\"outputs\", paste0(\"training_nested_\", COUNTRY_CODE, \".rds\")))\n",
    "```\n",
    "\n",
    "## 8) Next step (outside this notebook)\n",
    "\n",
    "- Pour **chaque sous-bassin**, vous avez maintenant **plusieurs data.frames (un par modèle)**.\n",
    "- Passez `training_nested[[HYBAS_ID]][[MODEL]]` à vos fonctions WASS2SHydroR.\n",
    "- Si besoin, vous pouvez concaténer au niveau modèle (empiler des années/féquences) ou faire de l’ensemblage plus tard.\n",
    "\n",
    "\n",
    "- Pass `training_list` directly to your WASS2SHydroR training / forecasting functions.\n",
    "- If you need cross-validation or lag engineering, do it on each element of the list.\n",
    "- To restrict to FULL-coverage subbasins only, filter `subs_sel$HYBAS_ID[subs_sel$coverage_class==\"FULL\"]` before step 2.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes & Design Choices (for instructors)\n",
    "\n",
    "- **Zero/low-code for trainees**: they change only `COUNTRY_CODE`, `PREDICTOR_VARS`, optionally `SELECTED_MODELS`.\n",
    "- **Model provenance in column names**: during assembly, we keep model tags (e.g., `prcp_CanCM3`) so you can compare models. Later we optionally remap to the strict `prcp_1`, `sst_1` pattern if required by WASS2SHydroR.\n",
    "- **Partial vs full coverage**: we compute area-based coverage ratios to help quality control.\n",
    "- **Parallelization**: a simple, notebook-friendly parallel helper is included; Windows falls back to sequential to avoid trainee issues.\n",
    "- **Adaptation hook**: if `wass2s_prepare_data()` already returns features (not pixels), the wrapper picks them; otherwise it aggregates pixel columns (mean). Replace `.aggregate_pixels()` if you prefer median, PCA, quantiles, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
